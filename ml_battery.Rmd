---
title: "Using Machine Learning Battery RCC and Oncocytoma Differentiation"
author: "Akshay Jaggi, Sandy Napel"
date: "16/01/2020"
output: pdf_document
---

```{r setup, include=FALSE}
setwd("/Users/akshay/Documents/Stanford/Research/Papers/RCC_CTTA/RCC_CTTA_Code/")
library(caret)
library(dplyr)
library(itertools)
library(mRMRe)
library(WilcoxCV)
library(datasets)
library(praznik)
library(kernlab)
library(stringr)
library(xgboost)
library(caretEnsemble)
library(dendextend)
library(gplots)
library(igraph)
library(GGally)
source("ml_battery_script.R")
```

Note: This code is run after "data_cleaning.Rmd"
This code will use the data frames generated by this code


# ICC Cutoff Optimization

Here is a list of ICC Cutoffs to Test
```{r set icc cutoffs}
icc_cutoffs = seq(0.5,0.95,by=0.05)
```

Parameters for the ML Battery Script can be specified here
You can choose the number of features to optimize for, the feature selection algorithms to try, the classifiers to use. If new things are added, they will be implemented under caret defaults unless otherwise specifed in code below. 
```{r set parameters for icc}
parameters_best = list(
  "features" = c(50), 
  "selections" = c("mRMR"), 
  "classifiers" = c("xgbLinear"))
```

This code will test the model performance on successive ICC Cutoffs 
(Generating Data for SupplementaryFigure S1)
```{r run icc cutoff experiment}
container = list()
for(cutoff in cutoffs) {
  to_keep = icc_cut(partial_iccs, cutoff)
  data_set_filt1 = dat_subset1[to_keep]
  colnames(data_set_filt1) = str_replace_all(colnames(data_set_filt1), "[^[:alnum:]]", ".")
  data_set_filt1$Label = data_subset_c1$Label
  data_set_filt2 = dat_subset2[to_keep]
  data_set_filt2$Label = data_subset_c2$Label
  colnames(data_set_filt2 ) = str_replace_all(colnames(data_set_filt2), "[^[:alnum:]]", ".")
  container[[as.character(cutoff)]] = 
    list("model1" = ml_battery_overhead(data_set_filt1, "Label", 5, parameters_best, 5),
         "model2" = ml_battery_overhead(data_set_filt2, "Label", 5, parameters_best, 5))
}
```

Here we write the results of this test to a csv usable in Prism
```{r print icc cutoff results}
results =  array(dim  = c(length(cutoffs),10))
i = 1
for(cont in container) {
  results[i,] = c(cont$model1$means[,,,,1],cont$model2$means[,,,,1])
  i = i + 1
}
write.csv(results, "icc_comparison_plot.csv")
```

# mRMR Size Optimization 

Set parameters for the ml battery script
Specify many mrmr tests sizes
```{r set mrmr parameters}
mrmr_test = list(
  "features" = c(5,10,20,30,40,50,60,70,80,90), 
  "selections" = c("mRMR"), 
  "classifiers" = c("xgbLinear"))
```

Run ICC Filtering with a 0.8 threshold
```{r use point eight icc cutoff}
to_keep = icc_cut(partial_iccs, 0.8)

data_set_filt1 = dat_subset1[to_keep]
colnames(data_set_filt1) = str_replace_all(colnames(data_set_filt1), "[^[:alnum:]]", ".")
data_set_filt1$Label = data_subset_c1$Label

data_set_filt2 = dat_subset2[to_keep]
data_set_filt2$Label = data_subset_c2$Label
colnames(data_set_filt2 ) = str_replace_all(colnames(data_set_filt2), "[^[:alnum:]]", ".")
```

Run the models
```{r run the mrmr test model}
model1_xgb_mrmr = ml_battery_overhead(data_set_filt1, "Label", 5, mrmr_test, 5)
model2_xgb_mrmr = ml_battery_overhead(data_set_filt2, "Label", 5, mrmr_test, 5)
```

Write their results
```{r write mrmr results}
write.csv(t(model1_xgb_mrmr$means[,,1,1,1]),"many_xg_mrmr1.csv")
write.csv(t(model2_xgb_mrmr$means[,,1,1,1]),"many_xg_mrmr2.csv")
```

# Train and Test the Optimized Model

Set the parameters for the final ML Battery Usage
```{r final parameters}
parameters_final = list(
  "features" = c(50), 
  "selections" = c("mRMR"), 
  "classifiers" = c("xgbLinear"))
```

Run the ML Battery with Five Train test splits 
```{r run the final models}
model1_xgb = ml_battery_overhead(data_set_filt1, "Label", 5, parameters_final, 5)
model2_xgb = ml_battery_overhead(data_set_filt2, "Label", 5, parameters_final, 5)
```


# Run Feature Importance Analysis

Define some functions useful for doing the feature importance analysis

```{r plot clustered importancs}
# plot_importances_clustered
# Generate the HeatMap and Importance Plots for each Model
# Takes:    the importance array generated by the importance voting function
#           the list of feature values used to train the model
#           the number of desired clusters
# Does:     Find the correlation between all selected features
#           Use this to generate a heat map and a clustering of the features
#           Collect the number of mrmr votes and the mean importances
#           Plot all this
# Returns:  Importance array
plot_importances_clustered = function(importance_array, features, clusts) {
  selected_feat_cor = cor(
    features[,colnames(features) %in% colnames(importance_array)])
  coloring = colored_hmap(selected_feat_cor, clusts)
  cluster_colors = as.factor(coloring[order(coloring$label),]$cluster)
  cluster_labels = as.character(c(1:length(levels(cluster_colors))))
  
  collected_importances = data.frame(
    "votes" = apply(!is.na(importance_array),2,sum),
    "means" = apply(importance_array,2,mean,na.rm=TRUE),
    "sd"    = apply(importance_array,2,sd,na.rm=TRUE))
  collected_importances = collected_importances[order(row.names(collected_importances)),]
  collected_importances$colors = cluster_colors
  collected_importances$clusters = cluster_colors
  # PROVISIONALLY CHANGE THE COLOR ORDERING TO IMPROVE PLOT APPEARANCE
  levels(collected_importances$clusters) = c(2,3,4,1,5)
  
  print(ggplot(collected_importances, 
                aes(
                  x = collected_importances$votes,
                  y = collected_importances$means,
                  color = collected_importances$cluster)) +
           labs( x = "Number of Votes", y = "Average Importance", color = "Cluster Number") +  
           geom_point(size = 2) + 
           theme_classic() + 
          scale_color_manual(labels = levels(collected_importances$clusters),
                             values = levels(collected_importances$colors)) + 
  theme(
    aspect.ratio = 1, 
    axis.title.x = element_text(size=14, face="bold", colour = "black"),    
    axis.title.y = element_text(size=14, face="bold", colour = "black"),    
    axis.text.x = element_text(size=14, face="bold", colour = "black"), 
    axis.text.y = element_text(size=14, face="bold", colour = "black"),
    axis.line.x = element_line(color="black", size = 1),
    axis.line.y = element_line(color="black", size = 1),
    legend.title = element_text(size=16,  face="bold", colour = "black"),
    legend.text = element_text(size=14,  colour = "black")))
  return(collected_importances)
}
```

```{r plot colored hmap}
# plot_importances_clustered
# Generate the colored heatmap
# Takes:    feature correlations, desired number of clusters
# Does:     Generate the heatmap from the correlation data
#           Conduct the clustering and dendrogram generation
#           Produce the heatmap
# Returns:  Coloring of all the features by cluster
colored_hmap = function(selected_feat_cor, clusts) {
  my_palette = colorRampPalette(c("red", "white", "forestgreen"))(n = 299)
  distance= dist(selected_feat_cor, method ="euclidean") 
  hcluster = hclust(distance)
  dend = as.dendrogram(hcluster)
  cols_branches = gg_color_hue(clusts)
  dend = color_branches(dend, k = clusts, col = cols_branches)
  labels = get_leaves_attr(dend,attribute = "label")
  col_labels = get_leaves_branches_col(dend)
  coloring = data.frame("label" = labels, "cluster" = col_labels)
  col_labels = col_labels[order(order.dendrogram(dend))]
  heatmap.2(selected_feat_cor,
            key = TRUE,
            key.title = NULL,
            key.xlab = "correlation",
            density.info = "none",
            trace="none",
            col=my_palette,
            dendrogram="row",
            Rowv = dend,
            Colv = dend,
            na.rm = TRUE,
            labCol = FALSE,
            labRow = FALSE,
            RowSideColors = col_labels,
            margins = c(0, 15))
  return(coloring)
}

# Generates evenly spaced colors around the color wheel like ggplot
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
```

```{r}
model1_xgb_importances = majority_vote_overhead(model1_xgb_4$importances)
model1_xgb_importances = model1_xgb_importances$`50`$mRMR$xgbLinear
collected_model1 = plot_importances_clustered_new(model1_xgb_importances, data_set_filt1, 5)
```

```{r}
model2_xgb_importances = majority_vote_overhead(model2_xgb_4$importances)
model2_xgb_importances = model2_xgb_importances$`50`$mRMR$xgbLinear
collected_model2 = plot_importances_clustered_new(model2_xgb_importances, data_set_filt2, 5)
```


# Run Cluster Comparison Between Rounds of Biopsies
```{r}
big_length = length(levels(collected_model1$cluster)) * length(levels(collected_model1$cluster))
source = vector(mode = "character", length = big_length)
target = vector(mode = "character", length = big_length)
weight = vector(mode = "numeric", length = big_length)
count = 1
for(level1 in levels(collected_model1$cluster)) {
  strings1 = row.names(collected_model1[collected_model1$cluster == level1,])
  for(level2 in levels(collected_model2$cluster)) {
    strings2 = row.names(collected_model2[collected_model2$cluster == level2,])
    in_common = length(intersect(strings1,strings2))/
      min(length(strings1),length(strings2))
    print(sprintf("Group %s and Group %s are %f similar", 
                  level1, level2, in_common))
    source[count] = level1 #paste(level1,"1",sep="_")
    target[count] = level2 #paste(level2,"2",sep="_")
    weight[count] = in_common
    count = count + 1
  }
}

links = data.frame(source, target, weight)
links$weighted = 7.5 * links$weight
```




